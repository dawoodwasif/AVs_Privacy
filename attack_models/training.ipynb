{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-12T06:28:50.045109Z","iopub.status.busy":"2024-02-12T06:28:50.044776Z","iopub.status.idle":"2024-02-12T06:28:55.954606Z","shell.execute_reply":"2024-02-12T06:28:55.953523Z","shell.execute_reply.started":"2024-02-12T06:28:50.045061Z"},"trusted":true},"outputs":[],"source":["# Import the Roboflow and YOLO modules\n","from roboflow import Roboflow\n","from ultralytics import YOLO\n","import random\n","import os\n","import numpy as np\n","\n","os.environ['WANDB_DISABLED'] = 'true'\n","os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n","\n","# The 'Roboflow' module is used for working with datasets on Roboflow platform\n","# The 'YOLO' module is imported from 'ultralytics' library, which is used for YOLO object detection tasks"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-12T06:28:55.956651Z","iopub.status.busy":"2024-02-12T06:28:55.956066Z","iopub.status.idle":"2024-02-12T06:29:05.237722Z","shell.execute_reply":"2024-02-12T06:29:05.236809Z","shell.execute_reply.started":"2024-02-12T06:28:55.956613Z"},"trusted":true},"outputs":[],"source":["# # Import the Roboflow library and create an instance with the provided API key\n","# api_key = \"4DcIf06sr4RzL8YOksef\"\n","# rf = Roboflow(api_key)\n","\n","# # Access the Roboflow workspace named \"lazydevs\"\n","# # Access the project named \"human-detection\" within the workspace\n","# workspace_name = \"lazydevs\"\n","# project_name = \"human-dectection\"\n","# project = rf.workspace(workspace_name).project(project_name)\n","\n","# # Download the dataset associated with version 4 of the project using YOLOv8 format\n","# # Note: You might want to include a specific version number or method for version selection.\n","# version = 4\n","# form = \"yolov8\"\n","# dataset = project.version(version).download(form)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-12T06:29:05.240144Z","iopub.status.busy":"2024-02-12T06:29:05.239758Z","iopub.status.idle":"2024-02-12T06:29:05.919845Z","shell.execute_reply":"2024-02-12T06:29:05.918868Z","shell.execute_reply.started":"2024-02-12T06:29:05.240108Z"},"trusted":true},"outputs":[],"source":["# Initialize the YOLO model by loading the pre-trained weights from 'yolov8n.pt'\n","model = YOLO('yolov8n.pt')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-12T06:29:05.922683Z","iopub.status.busy":"2024-02-12T06:29:05.922254Z","iopub.status.idle":"2024-02-12T08:45:06.455314Z","shell.execute_reply":"2024-02-12T08:45:06.453701Z","shell.execute_reply.started":"2024-02-12T06:29:05.922648Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["New https://pypi.org/project/ultralytics/8.2.70 available üòÉ Update with 'pip install -U ultralytics'\n","Ultralytics YOLOv8.2.66 üöÄ Python-3.10.14 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=Human-Dectection-4/data.yaml, epochs=5, time=None, patience=100, batch=64, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train25, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train25\n"]},{"name":"stdout","output_type":"stream","text":["Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n","Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n","\n","Transferred 319/355 items from pretrained weights\n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"name":"stderr","output_type":"stream","text":["`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/AVs_Privacy/attack_models/Human-Dectection-4/train/labels.cache... 4200 images, 48 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4200/4200 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /home/AVs_Privacy/attack_models/Human-Dectection-4/valid/labels.cache... 400 images, 1 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<?, ?it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Plotting labels to runs/detect/train25/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/detect/train25\u001b[0m\n","Starting training for 5 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        1/5      10.5G      1.467      1.882      1.696        214        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:26<00:00,  2.48it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all        400       1081      0.491      0.301      0.304      0.105\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        2/5      9.18G      1.426      1.615      1.678        191        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:27<00:00,  2.38it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.10it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        400       1081      0.348      0.383      0.265     0.0909\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        3/5      9.57G      1.432      1.564      1.689        199        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:20<00:00,  3.29it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.91it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        400       1081      0.278      0.281      0.189     0.0628\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        4/5      9.37G      1.395      1.494      1.659        178        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:20<00:00,  3.23it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.01it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        400       1081      0.609       0.53      0.565      0.228\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        5/5       9.7G      1.292      1.348       1.58        202        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:19<00:00,  3.31it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:01<00:00,  2.08it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        400       1081      0.721      0.576      0.659      0.315\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","5 epochs completed in 0.036 hours.\n","Optimizer stripped from runs/detect/train25/weights/last.pt, 6.2MB\n","Optimizer stripped from runs/detect/train25/weights/best.pt, 6.2MB\n","\n","Validating runs/detect/train25/weights/best.pt...\n","Ultralytics YOLOv8.2.66 üöÄ Python-3.10.14 torch-2.4.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3090, 24260MiB)\n","Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"]},{"name":"stderr","output_type":"stream","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all        400       1081      0.722      0.575      0.659      0.316\n","Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 1.4ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train25\u001b[0m\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>lr/pg1</td><td>‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>lr/pg2</td><td>‚ñÉ‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>metrics/mAP50(B)</td><td>‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñà</td></tr><tr><td>metrics/mAP50-95(B)</td><td>‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>metrics/precision(B)</td><td>‚ñÑ‚ñÇ‚ñÅ‚ñÜ‚ñà</td></tr><tr><td>metrics/recall(B)</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñá‚ñà</td></tr><tr><td>model/GFLOPs</td><td>‚ñÅ</td></tr><tr><td>model/parameters</td><td>‚ñÅ</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>‚ñÅ</td></tr><tr><td>train/box_loss</td><td>‚ñà‚ñÜ‚ñá‚ñÖ‚ñÅ</td></tr><tr><td>train/cls_loss</td><td>‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>train/dfl_loss</td><td>‚ñà‚ñá‚ñà‚ñÜ‚ñÅ</td></tr><tr><td>val/box_loss</td><td>‚ñÜ‚ñÜ‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>val/cls_loss</td><td>‚ñà‚ñá‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>val/dfl_loss</td><td>‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00042</td></tr><tr><td>lr/pg1</td><td>0.00042</td></tr><tr><td>lr/pg2</td><td>0.00042</td></tr><tr><td>metrics/mAP50(B)</td><td>0.65856</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.31577</td></tr><tr><td>metrics/precision(B)</td><td>0.72151</td></tr><tr><td>metrics/recall(B)</td><td>0.57519</td></tr><tr><td>model/GFLOPs</td><td>8.194</td></tr><tr><td>model/parameters</td><td>3011043</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.359</td></tr><tr><td>train/box_loss</td><td>1.29162</td></tr><tr><td>train/cls_loss</td><td>1.34825</td></tr><tr><td>train/dfl_loss</td><td>1.57987</td></tr><tr><td>val/box_loss</td><td>1.71542</td></tr><tr><td>val/cls_loss</td><td>1.37508</td></tr><tr><td>val/dfl_loss</td><td>1.86792</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/AVs_Privacy/attack_models/wandb/offline-run-20240731_063148-s58cotq4<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20240731_063148-s58cotq4/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Train the model with the specified parameters\n","\n","results = model.train(\n","    data='Human-Dectection-4/data.yaml',  # Path to the training data YAML file\n","    epochs=25,  # Number of training epochs\n","    batch=64,  # Batch size for training\n","    # imgsz=640,  # Input image size\n","    # seed=32,  # Random seed for reproducibility\n","    # optimizer='NAdam',  # Optimizer algorithm\n","    # weight_decay=1e-4,  # Weight decay for regularization\n","    # momentum=0.937,  # Initial momentum for the optimizer\n","    # cos_lr=True,  # Use cosine learning rate scheduling\n","    # lr0=0.01,  # Initial learning rate\n","    # lrf=1e-5,  # Final learning rate\n","    # warmup_epochs=10,  # Number of warmup epochs\n","    # warmup_momentum=0.5,  # Momentum during warm-up epochs\n","    # close_mosaic=20,  # Parameter for close mosaic augmentation\n","    # label_smoothing=0.2,  # Label smoothing parameter for regularization\n","    # dropout=0.5,  # Dropout rate to prevent overfitting\n","    verbose=True  # Print verbose training information\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_subfolders(folder_path):\n","    return [f.name for f in os.scandir(folder_path) if f.is_dir()]\n","\n","def load_data_pairs(image_dir, label_dir):\n","    images = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')])\n","    labels = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.txt')])\n","    return list(zip(images, labels))\n","\n","def create_shadow_datasets_from_paths(base_path):\n","    shadow_datasets = []\n","    shadow_dirs = get_subfolders(base_path)\n","    \n","    for shadow_dir in shadow_dirs:\n","        shadow_path = os.path.join(base_path, shadow_dir)\n","        train_data = load_data_pairs(os.path.join(shadow_path, 'train/images'), os.path.join(shadow_path, 'train/labels'))\n","        val_data = load_data_pairs(os.path.join(shadow_path, 'val/images'), os.path.join(shadow_path, 'val/labels'))\n","        test_data = load_data_pairs(os.path.join(shadow_path, 'test/images'), os.path.join(shadow_path, 'test/labels'))\n","        shadow_datasets.append({'train': train_data, 'val': val_data, 'test': test_data})\n","    \n","    return shadow_datasets"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["base_path = '/home/AVs_Privacy/attack_models/Human-Dectection-4/shadow_datasets'\n","shadow_datasets = create_shadow_datasets_from_paths(base_path)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def train_shadow_model(data_yaml, epochs=2, img_size=640):\n","    model = YOLO('yolov8n.pt')\n","    model.train(data=data_yaml, epochs=epochs, imgsz=img_size)\n","    return model\n","\n","def generate_attack_data(shadow_model, shadow_train_data, shadow_test_data):\n","    X_attack = []\n","    y_attack = []\n","\n","    def process_data(data, label):\n","        for img_path, _ in data:\n","            results = shadow_model(img_path)\n","\n","            c_conf = 0\n","            pred_vector = np.array([0,0,0,0,c_conf])\n","            \n","            \n","            ##########            \n","            for box in results[0].boxes:\n","            \n","                x1, y1, x2, y2 = box.xyxy.tolist()[0]\n","                conf = box.conf.item()\n","                if conf > c_conf:\n","                    c_conf = conf\n","                    pred_vector = np.array([x1, y1, x2, y2, conf])\n","            ##########\n","\n","            X_attack.append(pred_vector)\n","            y_attack.append(label)\n","\n","    # Process in-sample data (members)\n","    process_data(shadow_train_data, 1)\n","    \n","    # Process out-of-sample data (non-members)\n","    process_data(shadow_test_data, 0)\n","    \n","    return X_attack, y_attack\n","\n","attack_data_X = []\n","attack_data_y = []\n","\n","for i, dataset in enumerate(shadow_datasets):\n","    data_yaml = os.path.join(base_path, f'shadow_{i+1}.yaml')\n","    shadow_model = train_shadow_model(data_yaml)\n","    X, y = generate_attack_data(shadow_model, dataset['train'], dataset['test'])\n","    attack_data_X.extend(X)\n","    attack_data_y.extend(y)\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Attack Model Accuracy: 0.89106529209622\n"]}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Convert attack data to numpy arrays\n","X_attack = np.array(attack_data_X)\n","y_attack = np.array(attack_data_y)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_attack, y_attack, test_size=0.2, random_state=42)\n","\n","# Train the attack model\n","attack_model = RandomForestClassifier()\n","attack_model.fit(X_train, y_train)\n","\n","# Evaluate the attack model\n","print(\"Attack Model Accuracy:\", attack_model.score(X_test, y_test))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","image 1/1 /home/Virginia_Research/FACET/images/trainA/sa_4725.jpg: 448x640 1 Human, 7.5ms\n","Speed: 2.0ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n","0.83\n","Is member: True\n"]}],"source":["def infer_membership(target_model, attack_model, img_path):\n","    results = target_model(img_path)\n","    \n","    c_conf = 0\n","    pred_vector = np.array([0,0,0,0,c_conf])\n","    \n","    for box in results[0].boxes:\n","        x1, y1, x2, y2 = box.xyxy.tolist()[0]\n","        conf = box.conf.item()\n","        if conf > c_conf:\n","            c_conf = conf\n","            pred_vector = np.array([x1, y1, x2, y2, conf])\n","    \n","    membership_prob = attack_model.predict_proba([pred_vector])[0, 1]\n","    print(membership_prob)\n","    return membership_prob > 0.5  # Threshold for membership inference\n","\n","# Example usage\n","#mia_path = \"/home/AVs_Privacy/attack_models/Human-Dectection-4/all/images/Image_8_jpg.rf.8c5ff407d9510df1801be78fc8fcc262.jpg\"\n","#mia_path = \"/home/AVs_Privacy/attack_models/Human-Dectection-4/all/images/sa_5773423.jpg\"\n","#mia_path = \"/home/Virginia_Research/Federated_Learning/Custom_YOLO_KITTI/images/all/000002.jpg\"\n","mia_path = \"/home/Virginia_Research/FACET/images/trainA/sa_4725.jpg\"\n","\n","is_member = infer_membership(model, attack_model, mia_path)\n","print(\"Is member:\", is_member)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4421871,"sourceId":7596647,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
